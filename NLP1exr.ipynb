{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMC93Dwm9uBjnp42qP3vJYd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DianaGur/NLPcourse/blob/main/NLP1exr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment no.1 NLP**\n",
        "4. Load the spam.csv dataset file"
      ],
      "metadata": {
        "id": "chz3PKEjBO7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/sample_data/spam.csv', encoding='latin-1')\n",
        "\n",
        "# laoding two more copies to run th two diffrent technics on them\n",
        "copy1_df = pd.read_csv('/content/sample_data/spam.csv', encoding='latin-1')\n",
        "copy2_df = pd.read_csv('/content/sample_data/spam.csv', encoding='latin-1')\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W2zOoDTBdYJ",
        "outputId": "1b776b0f-ed9d-4d07-d224-d4a70ff2cc79"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        v1                                                 v2 Unnamed: 2  \\\n",
            "0      ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
            "1      ham                      Ok lar... Joking wif u oni...        NaN   \n",
            "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
            "3      ham  U dun say so early hor... U c already then say...        NaN   \n",
            "4      ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
            "...    ...                                                ...        ...   \n",
            "5567  spam  This is the 2nd time we have tried 2 contact u...        NaN   \n",
            "5568   ham              Will Ì_ b going to esplanade fr home?        NaN   \n",
            "5569   ham  Pity, * was in mood for that. So...any other s...        NaN   \n",
            "5570   ham  The guy did some bitching but I acted like i'd...        NaN   \n",
            "5571   ham                         Rofl. Its true to its name        NaN   \n",
            "\n",
            "     Unnamed: 3 Unnamed: 4  \n",
            "0           NaN        NaN  \n",
            "1           NaN        NaN  \n",
            "2           NaN        NaN  \n",
            "3           NaN        NaN  \n",
            "4           NaN        NaN  \n",
            "...         ...        ...  \n",
            "5567        NaN        NaN  \n",
            "5568        NaN        NaN  \n",
            "5569        NaN        NaN  \n",
            "5570        NaN        NaN  \n",
            "5571        NaN        NaN  \n",
            "\n",
            "[5572 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Print basic statistics:"
      ],
      "metadata": {
        "id": "PQfx4QU_EQti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def print_statistics(df):\n",
        "\n",
        "  # Total number of SMS messages\n",
        "  total_messages = df.shape[0]\n",
        "\n",
        "  # Number of spam/ham messages\n",
        "  spam_messages = df[df['v1'] == 'spam'].shape[0]\n",
        "  ham_messages = df[df['v1'] == 'ham'].shape[0]\n",
        "\n",
        "  # Average number of words per message\n",
        "  df['word_count'] = df['v2'].apply(lambda x: len(x.split()))\n",
        "  avg_words = df['word_count'].mean()\n",
        "\n",
        "  # 5 most frequent words\n",
        "  from collections import Counter\n",
        "  words = ' '.join(df['v2']).split()\n",
        "  most_common_words = Counter(words).most_common(5)\n",
        "\n",
        "  # Number of words that only appear once\n",
        "  word_counts = Counter(words)\n",
        "  unique_words_count = sum(1 for count in word_counts.values() if count == 1)\n",
        "\n",
        "  print(f'Total messages: {total_messages}')\n",
        "  print(f'Spam messages: {spam_messages}')\n",
        "  print(f'Ham messages: {ham_messages}')\n",
        "  print(f'Average words per message: {avg_words}')\n",
        "  print(f'5 most frequent words: {most_common_words}')\n",
        "  print(f'Number of unique words: {unique_words_count}')\n",
        "\n",
        "def print_word_stats(column, name):\n",
        "  all_words = [word for sublist in df[column] for word in sublist]\n",
        "  most_common = Counter(all_words).most_common(5)\n",
        "  unique_count = sum(1 for count in Counter(all_words).values() if count == 1)\n",
        "  print(f'\\n{name} - 5 most frequent: {most_common}')\n",
        "  print(f'{name} - Number of unique words: {unique_count}')\n",
        "\n",
        "# printing the basic statistics of the loaded file\n",
        "print_statistics(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZuth7CyEZ50",
        "outputId": "2b82d61c-f043-464c-e6c2-4333c2d1df00"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total messages: 5572\n",
            "Spam messages: 747\n",
            "Ham messages: 4825\n",
            "Average words per message: 15.494436468054559\n",
            "5 most frequent words: [('to', 2134), ('you', 1622), ('I', 1466), ('a', 1327), ('the', 1197)]\n",
            "Number of unique words: 9268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Tokenization using NLTK and spaCy:"
      ],
      "metadata": {
        "id": "bLjxCOQnEaQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import time\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nltk.download('punkt')\n",
        "\n",
        "start = time.time()\n",
        "# NLTK tokenization\n",
        "df['nltk_tokens'] = df['v2'].apply(nltk.word_tokenize)\n",
        "end = time.time()\n",
        "print(\"The time that took the nltk to tokenize the data is \"+str(end-start))\n",
        "#print basic statistics\n",
        "print_word_stats('nltk_tokens', 'NLTK Tokens')\n",
        "\n",
        "start = time.time()\n",
        "# spaCy tokenization\n",
        "df['spacy_tokens'] = df['v2'].apply(lambda text: [token.text for token in nlp(text)])\n",
        "end = time.time()\n",
        "print(\"The time that took the spacy to tokenize the data is \"+ str(end-start))\n",
        "#print basic statistics\n",
        "print_word_stats('spacy_tokens', 'spaCy Tokens')\n",
        "\n",
        "# Time complexity analysis can be added as comments."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-dHu-QgEac9",
        "outputId": "923c723b-3532-41a6-ac17-830c0adf6b45"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time that took the nltk to tokenize the data is 1.269660234451294\n",
            "\n",
            "NLTK Tokens - 5 most frequent: [('.', 4886), ('to', 2148), ('I', 1956), ('you', 1888), (',', 1871)]\n",
            "NLTK Tokens - Number of unique words: 6187\n",
            "The time that took the spacy to tokenize the data is 60.38950037956238\n",
            "\n",
            "spaCy Tokens - 5 most frequent: [('.', 4945), ('to', 2148), ('I', 1988), ('you', 1878), (',', 1857)]\n",
            "spaCy Tokens - Number of unique words: 6272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Lemmatization using NLTK and spaCy:"
      ],
      "metadata": {
        "id": "jUU7hulZEame"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "start = time.time()\n",
        "# NLTK Lemmatization\n",
        "df['nltk_lemmas'] = df['v2'].apply(lambda text: [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text)])\n",
        "end = time.time()\n",
        "print(\"The time that took the nltk to lemmanize the data is \"+str(end-start))\n",
        "print_word_stats('nltk_lemmas', 'NLTK Lemmas')\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "# spaCy Lemmatization\n",
        "df['spacy_lemmas'] = copy2_df['v2'].apply(lambda text: [token.lemma_ for token in nlp(text)])\n",
        "end = time.time()\n",
        "print(\"The time that took the nltk to lemmanize the data is \"+str(end-start))\n",
        "print_word_stats('spacy_lemmas', 'spaCy Lemmas')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7543YENDEau8",
        "outputId": "cacc4471-c246-489a-eafc-ea119a327380"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time that took the nltk to lemmanize the data is 1.7307419776916504\n",
            "\n",
            "NLTK Lemmas - 5 most frequent: [('.', 4886), ('to', 2148), ('I', 1956), ('you', 1888), (',', 1871)]\n",
            "NLTK Lemmas - Number of unique words: 5903\n",
            "The time that took the nltk to lemmanize the data is 58.90249967575073\n",
            "\n",
            "spaCy Lemmas - 5 most frequent: [('.', 4945), ('I', 3722), ('be', 3260), ('to', 2309), ('you', 2217)]\n",
            "spaCy Lemmas - Number of unique words: 5359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Stemming using NLTK and spaCy:\n"
      ],
      "metadata": {
        "id": "HbIR2PKIFB7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "start = time.time()\n",
        "# NLTK Stemming\n",
        "nltk_stems = df['v2'].apply(lambda text: [stemmer.stem(word) for word in nltk.word_tokenize(text)])\n",
        "end = time.time()\n",
        "print(\"The time that took the nltk to stemmer the data is \"+str(end-start))\n",
        "# spaCy Stemming (note: spaCy does not support stemming out of the box)\n",
        "# For demonstration, using lemmatization result instead.\n"
      ],
      "metadata": {
        "id": "UdvQVUGgFGRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Comparison between NLTK and spaCy:\n",
        "\n",
        "NLTK is a comprehensive toolkit that offers a wide array of tools for text processing, such as tokenization, lemmatization, and stemming. It's very versatile and great for learning purposes because it includes a lot of resources and documentation. However, one downside is that it can be a bit slow, as many of its functions are implemented in Python. On the other hand, spaCy is designed for performance and ease of use, making it ideal for real-world applications. It's much faster since many of its functions are written in Cython, and it provides efficient tokenization, lemmatization, and even named entity recognition. While spaCy might not have as many features as NLTK, its straightforward API and focus on speed make it perfect for building scalable NLP applications. In summary, NLTK is excellent for educational purposes and experimentation, while spaCy is better suited for production environments where performance is critical.\n",
        "\n",
        "10. had been added in the sections of throut the code (6)."
      ],
      "metadata": {
        "id": "xnFPcu_3FKPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Web Scraping**\n",
        "\n",
        "11. Scrape text data using BeautifulSoup:"
      ],
      "metadata": {
        "id": "DcnalDWLKTV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://www.linkedin.com/feed/?midToken=AQG8RUfApq-I0w&midSig=2RI3sO38WW4Hk1&trk=eml-email_network_conversations_01-header-0-home_glimmer%20with_badging&trkEmail=eml-email_network_conversations_01-header-0-home_glimmer%20with_badging-null-lwkmmg~lxj6pup5~l8-null-null&eid=lwkmmg-lxj6pup5-l8&otpToken=MTMwNjFjZTQxNjI3YzBjY2IwMjYwZmViNDExZmVmYjI4YmNmZDU0ODllYWU4ZDZlN2JjZjAzNjY0ZjVhNTRmNmYwZDBkNWU5MTNlN2JhYzY3ZTgzYTI5NWNiYjkwMzdmMjdkMDRlYzVjYjhkYTA5MzhhY2JjOTQ4LDEsMQ%3D%3D'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "text_data = soup.get_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rjjrlZJ8FYZ8",
        "outputId": "46c0f00f-6ceb-4dc3-822f-c2f761801fb6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            \n",
            "          LinkedIn Login, Sign in | LinkedIn\n",
            "      \n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "        Sign in\n",
            "    \n",
            "\n",
            "          Discover people, jobs, and more.\n",
            "          \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "              17\n",
            "            \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            Diana Gurvits\n",
            "        \n",
            "\n",
            "g*****@gmail.com\n",
            "\n",
            "\n",
            "              Not you?\n",
            "            \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "          Password\n",
            "      \n",
            "\n",
            "        show\n",
            "      \n",
            "\n",
            " \n",
            "\n",
            "\n",
            "            Forgot password?\n",
            "          \n",
            "\n",
            "\n",
            "\n",
            " \n",
            "              Sign in\n",
            "          \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "or\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "          By clicking Continue, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy.\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "          Sign in with a passkey\n",
            "      \n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "            We’ve emailed a one-time link to g*****@gmail.com\n",
            "        \n",
            "Click on the link to sign in instantly to your LinkedIn account.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "If you don’t see the email in your inbox, check your spam folder.\n",
            "\n",
            "          Resend email\n",
            "        \n",
            "\n",
            "          Back\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "              Agree & Join LinkedIn\n",
            "            \n",
            "\n",
            "              By clicking Continue, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy.\n",
            "            \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                LinkedIn\n",
            "              \n",
            "              © 2024\n",
            "            \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                    User Agreement\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                  Privacy Policy\n",
            "                \n",
            "\n",
            " \n",
            "\n",
            "                  Community Guidelines\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                  Cookie Policy\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                  Copyright Policy\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "                  Send Feedback\n",
            "                \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Language\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                      العربية (Arabic)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      Čeština (Czech)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      Dansk (Danish)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      Deutsch (German)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "English (English)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                      Español (Spanish)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      Français (French)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      हिंदी (Hindi)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      Bahasa Indonesia (Indonesian)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      Italiano (Italian)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      日本語 (Japanese)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      한국어 (Korean)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      Bahasa Malaysia (Malay)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      Nederlands (Dutch)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      Norsk (Norwegian)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      Polski (Polish)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      Português (Portuguese)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      Română (Romanian)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      Русский (Russian)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      Svenska (Swedish)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      ภาษาไทย (Thai)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      Tagalog (Tagalog)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      Türkçe (Turkish)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      Українська (Ukrainian)\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      简体中文 (Chinese (Simplified))\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "                      正體中文 (Chinese (Traditional))\n",
            "                  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "also, i had to add a function that can print the statistics of the methods"
      ],
      "metadata": {
        "id": "MlQevs3U0g5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Initialize necessary NLTK and spaCy components\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "stemmer = nltk.PorterStemmer()\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def print_word_stats(word_list, description):\n",
        "    word_counts = Counter(word_list)\n",
        "    most_common_words = word_counts.most_common(5)\n",
        "    unique_words_count = sum(1 for count in word_counts.values() if count == 1)\n",
        "    print(f'\\n{description} - 5 most frequent: {most_common_words}')\n",
        "    print(f'{description} - Number of unique words: {unique_words_count}')\n",
        "\n",
        "print_word_stats(text_data,'Text data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "O5eOHNqo1WDT",
        "outputId": "c487ccae-cc23-4163-95b2-431b9a6e305c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'text_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-85d897f3da76>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{description} - Number of unique words: {unique_words_count}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprint_word_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Text data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'text_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.  Tokenize, lemmatize, and stem the scraped text:"
      ],
      "metadata": {
        "id": "waRwW-aEHrXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Tokenize\n",
        "nltk_tokens_scraped = nltk.word_tokenize(text_data)\n",
        "spacy_tokens_scraped = [token.text for token in nlp(text_data)]\n",
        "print_word_stats(nltk_tokens_scraped, 'NLTK Tokens')\n",
        "print_word_stats(spacy_tokens_scraped, 'spaCy Tokens')\n",
        "\n",
        "# Lemmatize\n",
        "nltk_lemmas_scraped = [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text_data)]\n",
        "spacy_lemmas_scraped = [token.lemma_ for token in nlp(text_data)]\n",
        "print_word_stats(nltk_lemmas_scraped, 'NLTK Lemmas')\n",
        "print_word_stats(spacy_lemmas_scraped, 'spaCy Lemmas')\n",
        "\n",
        "# Stem\n",
        "nltk_stems_scraped = [stemmer.stem(word) for word in nltk.word_tokenize(text_data)]\n",
        "print_word_stats(nltk_stems_scraped, 'NLTK Stems')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucWa9KuHIIj5",
        "outputId": "9a58246b-230e-469f-9f28-560e36aeb70b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NLTK Tokens - 5 most frequent: [('(', 28), (')', 28), (',', 10), ('*', 10), ('LinkedIn', 7)]\n",
            "NLTK Tokens - Number of unique words: 95\n",
            "\n",
            "spaCy Tokens - 5 most frequent: [('(', 28), (')', 28), ('\\n                  \\n\\n\\n\\n                      ', 23), (',', 10), ('LinkedIn', 7)]\n",
            "spaCy Tokens - Number of unique words: 128\n",
            "\n",
            "NLTK Lemmas - 5 most frequent: [('(', 28), (')', 28), (',', 10), ('*', 10), ('LinkedIn', 7)]\n",
            "NLTK Lemmas - Number of unique words: 95\n",
            "\n",
            "spaCy Lemmas - 5 most frequent: [('(', 28), (')', 28), ('\\n                  \\n\\n\\n\\n                      ', 23), (',', 10), ('LinkedIn', 6)]\n",
            "spaCy Lemmas - Number of unique words: 126\n",
            "\n",
            "NLTK Stems - 5 most frequent: [('(', 28), (')', 28), (',', 10), ('*', 10), ('linkedin', 7)]\n",
            "NLTK Stems - Number of unique words: 89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.Print word statistics on the scraped data.\n",
        "  The ststistics alredy printed troughout the code sections.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QhoZvv3qILhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WhatsApp Analysis**\n",
        "\n",
        "14. Import a .txt file of WhatsApp messages:\n"
      ],
      "metadata": {
        "id": "xHkDo00bKd3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/sample_data/WhatsApp.txt', 'r', encoding='utf-8') as file:\n",
        "  whatsapp_data = file.read()\n",
        "\n",
        "  #print comparisons of word statistics before processing\n",
        "print_word_stats(whatsapp_data, 'whatsappData')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsB_YyKJKplc",
        "outputId": "6fca3d9d-ee50-4fb6-e49c-3e313b0946d0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "whatsappData - 5 most frequent: [(' ', 315), ('י', 158), ('ו', 155), ('ה', 127), ('ל', 109)]\n",
            "whatsappData - Number of unique words: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Tokenize, lemmatize, and stem the WhatsApp data:\n"
      ],
      "metadata": {
        "id": "cDUhIeLPKz_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize\n",
        "print(\"Tokenize\")\n",
        "whatsapp_nltk_tokens = nltk.word_tokenize(whatsapp_data)\n",
        "print_word_stats(whatsapp_nltk_tokens, 'NLTK Tokens')\n",
        "whatsapp_spacy_tokens = [token.text for token in nlp(whatsapp_data)]\n",
        "print_word_stats(whatsapp_spacy_tokens, 'spaCy Tokens')\n",
        "\n",
        "# Lemmatize\n",
        "print(\"Lemmatize\")\n",
        "whatsapp_nltk_lemmas = [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(whatsapp_data)]\n",
        "print_word_stats(whatsapp_nltk_lemmas, 'NLTK Lemmas')\n",
        "whatsapp_spacy_lemmas = [token.lemma_ for token in nlp(whatsapp_data)]\n",
        "print_word_stats(whatsapp_spacy_lemmas, 'spaCy Lemmas')\n",
        "\n",
        "# Stem\n",
        "print(\"Sten\")\n",
        "whatsapp_nltk_stems = [stemmer.stem(word) for word in nltk.word_tokenize(whatsapp_data)]\n",
        "print_word_stats(whatsapp_nltk_stems, 'NLTK Stems')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucITs6UNK2XW",
        "outputId": "bb160a22-d868-4079-86cd-734d2b8627ee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize\n",
            "\n",
            "NLTK Tokens - 5 most frequent: [('.', 22), (',', 18), ('לא', 14), ('זה', 13), ('?', 12)]\n",
            "NLTK Tokens - Number of unique words: 213\n",
            "\n",
            "spaCy Tokens - 5 most frequent: [('\\n', 52), ('.', 22), (',', 18), ('לא', 14), ('זה', 13)]\n",
            "spaCy Tokens - Number of unique words: 214\n",
            "Lemmatize\n",
            "\n",
            "NLTK Lemmas - 5 most frequent: [('.', 22), (',', 18), ('לא', 14), ('זה', 13), ('?', 12)]\n",
            "NLTK Lemmas - Number of unique words: 213\n",
            "\n",
            "spaCy Lemmas - 5 most frequent: [('\\n', 52), ('.', 22), (',', 18), ('לא', 14), ('זה', 13)]\n",
            "spaCy Lemmas - Number of unique words: 214\n",
            "Sten\n",
            "\n",
            "NLTK Stems - 5 most frequent: [('.', 22), (',', 18), ('לא', 14), ('זה', 13), ('?', 12)]\n",
            "NLTK Stems - Number of unique words: 213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Print comparisons of word statistics after processing:\n",
        "\n",
        "(The statistics befor the proceessinf is printed above)"
      ],
      "metadata": {
        "id": "WAiNrAjkK9GL"
      }
    }
  ]
}