# -*- coding: utf-8 -*-
"""NLP2exr.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yn0XxnMV09tF8XV53W92yskgt5q0uIHi

**Assignment no.2 NLP**

1. First I Load the dataset file to the code.
"""

import pandas as pd
import nltk
import spacy
from collections import Counter
import os

# Load the dataset
df = pd.read_csv('/content/sample_data/spam.csv', encoding='latin-1')

text_column = 'v2'
text = ' '.join(df[text_column].astype(str).tolist())

print(df)

"""Help function to print basic statistics of the dataset:"""

def print_statistics(df):

  # Total number of SMS messages
  total_messages = df.shape[0]

  # Number of spam/ham messages
  spam_messages = df[df['v1'] == 'spam'].shape[0]
  ham_messages = df[df['v1'] == 'ham'].shape[0]

  # Average number of words per message
  df['word_count'] = df['v2'].apply(lambda x: len(x.split()))
  avg_words = df['word_count'].mean()

  # 5 most frequent words
  from collections import Counter
  words = ' '.join(df['v2']).split()
  most_common_words = Counter(words).most_common(5)

  # Number of words that only appear once
  word_counts = Counter(words)
  unique_words_count = sum(1 for count in word_counts.values() if count == 1)

  print(f'Total messages: {total_messages}')
  print(f'Spam messages: {spam_messages}')
  print(f'Ham messages: {ham_messages}')
  print(f'Average words per message: {avg_words}')
  print(f'5 most frequent words: {most_common_words}')
  print(f'Number of unique words: {unique_words_count}')

def print_word_stats(column, name):
  all_words = [word for sublist in df[column] for word in sublist]
  most_common = Counter(all_words).most_common(5)
  unique_count = sum(1 for count in Counter(all_words).values() if count == 1)
  print(f'\n{name} - 5 most frequent: {most_common}')
  print(f'{name} - Number of unique words: {unique_count}')

# printing the basic statistics of the loaded file
print_statistics(df)

"""2. Tokenization :"""

import nltk
nltk.download('punkt')
from nltk.tokenize import regexp_tokenize
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize

# Using split for white space tokenization
whitespace_tokens = text.split()
print(whitespace_tokens)

# Regex tokeniztion
pattern = r'\w+'
regex_tokens = regexp_tokenize(text, pattern)
print(regex_tokens)

# Word tokenizer
word_tokens = word_tokenize(text)
print(word_tokens)

# Sentence tokenizer
sentence_tokens = sent_tokenize(text)
print(sentence_tokens)

"""3. Apply Normalization:

3.1) Stemming
"""

from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

# Stemming for white space tokenization
stemmed_whitespace_tokens = [stemmer.stem(token) for token in whitespace_tokens]
print(stemmed_whitespace_tokens)


# Stemming Regex tokeniztion
stemmed_regex_tokens = [stemmer.stem(token) for token in regex_tokens]
print(stemmed_regex_tokens)

# Stemming word tokenizer
stemmed_word_tokens = [stemmer.stem(token) for token in word_tokens]
print(stemmed_word_tokens)

# Stemming sentence tokenizer
stemmed_sentence_tokens = [stemmer.stem(token) for token in sentence_tokens]
print(stemmed_sentence_tokens)

"""3.2) Applying lemmatization:"""

from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()
nltk.download('wordnet')

# Lemmatization for white space tokenization
lemmatized_whitespace_tokens = [lemmatizer.lemmatize(token) for token in word_tokens]
print(lemmatized_whitespace_tokens)

# Lemmatization Regex tokeniztion
lemmatized_regex_tokens = [lemmatizer.lemmatize(token) for token in word_tokens]
print(lemmatized_regex_tokens)

# Lemmatization Word tokenizer
lemmatized_word_tokens = [lemmatizer.lemmatize(token) for token in word_tokens]
print(lemmatized_word_tokens)

# Lemmatization Sentence tokenizer
lemmatized_sentence_tokens = [lemmatizer.lemmatize(token) for token in sentence_tokens]
print(lemmatized_sentence_tokens)

"""4. Remove Stop Words"""

from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')

#Removing stop words from word tokeneztion
stop_words = set(stopwords.words('english'))
filtered_tokens = [token for token in word_tokens if token.lower() not in stop_words]
print(filtered_tokens)

#Removing stop words from word tokeneztion
stop_words = set(stopwords.words('english'))
filtered_tokens = [token for token in word_tokens if token.lower() not in stop_words]
print(filtered_tokens)

#Removing stop words from word tokeneztion
stop_words = set(stopwords.words('english'))
filtered_tokens = [token for token in word_tokens if token.lower() not in stop_words]
print(filtered_tokens)

#Removing stop words from word tokeneztion
stop_words = set(stopwords.words('english'))
filtered_tokens = [token for token in word_tokens if token.lower() not in stop_words]
print(filtered_tokens)

"""Step 5: Apply Feature Extraction

a. BOW (Bag of Words)
"""

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
X_bow = vectorizer.fit_transform([text])
print(X_bow.toarray())
print(vectorizer.get_feature_names_out())

"""
b. TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer()
X_tfidf = tfidf_vectorizer.fit_transform([text])
print(X_tfidf.toarray())
print(tfidf_vectorizer.get_feature_names_out())

"""c. Word embedding by WORD2VEC"""

from gensim.models import Word2Vec

# Tokenize the text into sentences, then words
sentences = [word_tokenize(sentence) for sentence in sent_tokenize(text)]
word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)
word_vectors = word2vec_model.wv
print(word_vectors['remember'])

"""6. Glove (Global Vectors for Word Representation) is an unsupervised learning algorithm for obtaining vector representations for words. Unlike Word2Vec, which learns vectors based on local context windows in sentences, Glove learns word vectors by aggregating global word-word co-occurrence statistics from a corpus.

To apply Glove to your data, you would typically need pre-trained Glove vectors. You can download them from the official website.

"""

import numpy as np

# Load the pre-trained GloVe vectors
glove_path = '/content/sample_data/glove.6B.50d.txt'
embeddings_index = {}
with open(glove_path, 'r', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

# Example usage
word = 'remember'
print(embeddings_index[word])

"""7. *Select 5 sentences and apply to them Tagging by CYK*

The CYK (Cocke-Younger-Kasami) algorithm is used for parsing sentences in natural language processing. You would need a grammar and the nltk library for this task.



"""

import nltk
from nltk import CFG

# Define a simple grammar
grammar = CFG.fromstring("""
S -> NP VP
VP -> V NP | V NP PP
PP -> P NP
V -> "are" | "going" | "asked" | "call" | "get" | "know" | "told" | "finished"
NP -> "class" | "you" | "nothing" | "great" | "him" | "ok" | "immunisation" | "nigeria" | "friend" | "that" | "hep" | "b"
Det -> "a" | "an" | "the" | "my"
P -> "in"
""")

# Create the parser
parser = nltk.ChartParser(grammar)

# Provided sentences
sentences = [
    "Finished class where are you",
    "Going on nothing great bye",
    "I asked you to call him now ok",
    "Didn't you get hep b immunisation in nigeria",
    "I know that my friend already told that"
]

# Apply parsing
for sentence in sentences:
    words = sentence.lower().split()
    print(f"Parsing sentence: {sentence}")
    try:
        for tree in parser.parse(words):
            print(tree)
    except ValueError as e:
        print(f"Error parsing sentence: {e}")
    print()